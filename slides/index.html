<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Slides - Myreze-Labs: Depth Estimation</title>
    <link rel="stylesheet" href="reveal.js/dist/reset.css">
    <link rel="stylesheet" href="reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="reveal.js/dist/theme/black.css">
    <link rel="stylesheet" href="../css/slides-custom.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <!-- Title Slide -->
            <section>
                <div class="title-slide">
                    <img src="../images/myreze_logo.svg" alt="Myreze Logo" class="logo-medium">
                    <h1>Depth Estimation</h1>
                    <h3>Workshop by Myreze-Labs</h3>
                </div>
            </section>

            <!-- Workshop Overview -->
            <section>
                <h2>Workshop Overview</h2>
                <ul>
                    <li>Machine Learning Basics</li>
                    <li>Licensing & Models</li>
                    <li>3D Deep Learning</li>
                    <li>Depth Estimation Concepts</li>
                    <li>Point Cloud Meshing</li>
                    <li>Interactive Exercises</li>
                    <li>ComfyUI Workflows</li>
                    <li>Practical Applications</li>
                </ul>
            </section>

            <!-- Machine Learning Section -->
            <section data-background-color="#FFFFE6">
                <img src="visuals/paradigm.png" width="80%"">
                <small>
                    <font color=" #262a24"> Machine learning - a programming paradigm </font>
                </small>
                <small>
                    <font color="#262a24">Chollet, F. (2021). Deep learning with Python. Simon and Schuster.</font>
                </small>
            </section>

            <section data-background-color="#FFFFE6">
                <img src="visuals/regression_model_6.png" width="80%"">
            </section>

            <section data-background-color=" #181a18" style="text-align:left;">
                <font color="#ffffcc">How is AI/ML typically used? </font>

                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Detecting features and patterns </p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Categorize stuff</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Cluster similar things together</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Predicting coming elements in a sequence</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Determine policies (reinforcement learning)
                </p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Generate stuff</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Generate expressive representations</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Mimic various mechanisms and things</p>
                <p></p>
            </section>




            <section
                data-background-iframe="https://www.esa.int/Applications/Observing_the_Earth/Ph-sat/Next_artificial_intelligence_mission_selected"
                data-background-interactive>
                <font color="#ffffcc"> Detecting features and patterns</font>
            </section>


            <section data-background-color="#181a18">
                <font color="#ffffcc">Categorization</font>
                <div>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/ujNfhu-150A?si=4VBjin39oTLuxqcJ"
                        title="YouTube video player" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                        referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                </div>
            </section>

            <section data-background-color="#181a18">
                <font color="#ffffcc"> Clustering</font>
                <img src="visualscl/clustering.png">
                <small>Image from:
                    https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68
                </small>
            </section>

            <section data-background-image="visuals/blloch_2.png" data-background-opacity=1.0
                data-background-size="100%">
                <p>&nbsp;</p>
                <p>&nbsp;</p>
                <font color="#111166"> Series prediction</font>
            </section>


            <section data-background-color="#181a18">
                <font color="#ffffcc"> Determination of optimal policies</font>
            </section>


            <section data-background-image="visuals/hospital.png" data-background-size="contain">
                <p>&nbsp;</p>
                <p>&nbsp;</p>
                <p>&nbsp;</p>

                <div class="citation">
                    <p
                        style="font-size: 14px; background-color: rgba(0, 0, 0, 0.5); color: #fff; padding: 5px 10px; margin: 10px 0;">
                        Li, J., Wang, S., Zhang, M., Li, W., Lai, Y., Kang, X., ... & Liu, Y. (2024). Agent hospital: A
                        simulacrum
                        of hospital with evolvable medical agents. arXiv preprint arXiv:2405.02957.
                    </p>
                </div>
            </section>



            <section data-background-color="#181a18">
                <img src="visuals/OIG.jpeg" width="40%">
                <p>
                    <font color="#ffffcc"> Generating stuff</font>
                </p>

            </section>

            <section data-background-color="#181a18">
                <p>
                    <font color="#ffffcc"> Generating expressive representations</font>
                </p>
            </section>

            <section data-background-iframe="https://graceavery.com/word2vec-fish-music-bass/"
                data-background-interactive>
                <div
                    style="position: absolute; width: 70%; right: 0; box-shadow: 0 1px 4px rgba(0,0,0,0.5), 0 5px 25px rgba(0,0,0,0.2); background-color: rgba(0, 0, 0, 0.9); color: #fff; padding: 20px; font-size: 20px; text-align: left;">
                    Word2vec : Nelson, P., Urs, N. V., & Kasicheyanula, T.
                    R. (2022). Progress in Natural Language Processing and Language Understanding. In Bridging Human
                    Intelligence and Artificial Intelligence (pp. 83-103). Springer, Cham.
                </div>
            </section>

            <section data-background-video="visuals/sdf_grid_lq.mp4" data-background-video-loop="true"
                data-background-size="contain">
                <font color="#ffffcc">Regression (mimicking stuff)</font>
                <div class="citation">
                    <p
                        style="font-size: 14px; background-color: rgba(0, 0, 0, 0.5); color: #fff; padding: 5px 10px; margin: 10px 0;">
                        MÃ¼ller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives
                        with
                        a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4), 1-15.
                    </p>
                </div>
            </section>

            <section data-background-color=" #181a18" style="text-align:left;">
                <img src="visuals/extrapolation.png" width="80%">
            </section>

            <section data-background-color=" #181a18" style="text-align:left;">
                <font color="#ffffcc">When not to use it? </font>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Efficient analytical solutions exists </p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> More exact analytical solutions exists</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> When you lack sufficient data</p>
                <p></p>
            </section>


            <section data-background-color=" #181a18" style="text-align:left;">
                <font color="#ffffcc">Some 3D deep learning tasks </font>

                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Object detection and classification</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Object localization</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Shape reconstruction</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Scene understanding</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> 3D operations (registration, mappings etc.)
                </p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Generative models</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Multimodal LLMs</p>
                <p class="fragment"><i class="fas fa-kiwi-bird"></i> Representation (NERFs, Gaussian Splats, etc.)</p>
                <p></p>
            </section>

            <!-- Licensing Section -->
            <section>
                <section>
                    <h2>Licensing</h2>
                    <p>Understanding model licenses and constraints</p>
                </section>
                <section>
                    <h3>Common ML Model Licenses</h3>
                    <ul>
                        <li>MIT License</li>
                        <li>Apache License</li>
                        <li>Creative Commons</li>
                        <li>Commercial Licenses</li>
                    </ul>
                </section>
                <section>
                    <h3>Considerations</h3>
                    <ul>
                        <li>Usage rights</li>
                        <li>Distribution restrictions</li>
                        <li>Attribution requirements</li>
                        <li>Commercial vs. non-commercial use</li>
                    </ul>
                </section>
            </section>

            <!-- 3D Deep Learning Section -->
            <section>
                <section>
                    <h2>3D Deep Learning</h2>
                    <p>Current challenges and future prospects</p>
                </section>
                <section>
                    <h3>Current Challenges</h3>
                    <ul>
                        <li>Computational complexity</li>
                        <li>Data acquisition</li>
                        <li>Representation learning</li>
                        <li>Domain adaptation</li>
                    </ul>
                </section>
                <section>
                    <h3>Future Prospects</h3>
                    <ul>
                        <li>Real-time 3D reconstruction</li>
                        <li>Cross-domain understanding</li>
                        <li>Neural rendering</li>
                        <li>Generative 3D models</li>
                    </ul>
                </section>
            </section>

            <!-- Depth Estimation Section -->
            <section>
                <section>
                    <h2>Depth Estimation</h2>
                    <p>From RGB to RGBD</p>
                </section>
                <section>
                    <h3>What is an Image?</h3>
                    <ul>
                        <li>2D representation of visual information</li>
                        <li>RGB pixel values</li>
                        <li>Missing depth information</li>
                    </ul>
                </section>
                <section>
                    <img src="visuals/point_cloud.png" width="80%">
                    <h3>What is a Point Cloud?</h3>
                    <ul>
                        <li>3D coordinates in space</li>
                        <li>Represents object surfaces</li>
                        <li>Generated from depth data</li>
                    </ul>
                </section>
                <section>
                    <h3>Focal Length & Perspective</h3>
                    <ul>
                        <li>Camera parameters</li>
                        <li>Projection from 3D to 2D</li>
                        <li>Impact on depth perception</li>
                    </ul>
                </section>
            </section>

            <!-- Meshing of Point Clouds -->
            <section>
                <section>
                    <h2>Meshing of Point Clouds</h2>
                    <p>Converting points to surfaces</p>
                </section>
                <section>
                    <img src="https://www.researchgate.net/profile/Seung-Hyun-Yoon-3/publication/221209194/figure/fig2/AS:305597699379203@1449871577685/Approximation-process-a-point-cloud-b-control-mesh-c-result.png"
                        width="80%">
                    <h3>Meshing Techniques</h3>
                    <ul>
                        <li>Delaunay triangulation</li>
                        <li>Alpha shapes</li>
                        <li>Poisson surface reconstruction</li>
                    </ul>
                </section>
                <section>
                    <h3>RGB-RGBD Special Case</h3>
                    <p>Using RTIN (Right-Triangulated Irregular Networks)</p>
                    <ul>
                        <li>Optimized for depth maps</li>
                        <li>Hierarchical representation</li>
                        <li>Adaptive detail</li>
                    </ul>
                </section>
            </section>

            <!-- Mini Exercises -->
            <section>
                <section>
                    <h2>Mini Exercises</h2>
                    <p>Based on the examples in the example section</p>
                </section>
                <section>
                    <h3>Discussion Points</h3>
                    <ul>
                        <li>How does the approach limit the movement of the viewer?</li>
                        <li>Where does the model work and not work?</li>
                        <li>What does depth add to the information from a storytelling perspective?</li>
                    </ul>
                </section>
            </section>

            <!-- ComfyUI Overview -->
            <section>
                <section>
                    <h2>ComfyUI Overview</h2>
                    <p>Introduction to the ComfyUI workflow</p>
                </section>
                <section>
                    <h3>What is ComfyUI?</h3>
                    <ul>
                        <li>Node-based interface for AI image generation</li>
                        <li>Customizable workflows</li>
                        <li>Extensible with plugins</li>
                    </ul>
                </section>
                <section>
                    <h3>Basic Workflow</h3>
                    <ol>
                        <li>Input image</li>
                        <li>Apply depth estimation</li>
                        <li>Post-process depth map</li>
                        <li>Generate 3D visualization</li>
                    </ol>
                </section>
            </section>

            <!-- Custom Models and Workflows -->
            <section>
                <section>
                    <h2>Custom Models and Workflows</h2>
                    <p>Depth estimation options in ComfyUI</p>
                </section>
                <section>
                    <h3>ComfyUI-depth-fm</h3>
                    <ul>
                        <li><strong>Model:</strong> DepthFM</li>
                        <li><strong>Features:</strong> Fast, single-step, versatile</li>
                        <li><strong>Options:</strong> Steps, ensemble size</li>
                        <li><strong>Use Case:</strong> Quick depth maps for real-time apps</li>
                    </ul>
                </section>
                <section>
                    <h3>ComfyUI-DepthAnythingV2</h3>
                    <ul>
                        <li><strong>Model:</strong> Depth Anything V2</li>
                        <li><strong>Features:</strong> Fine-grained, robust, efficient</li>
                        <li><strong>Options:</strong> Blur radius, median size, resolution</li>
                        <li><strong>Use Case:</strong> Detailed 3D effects in digital art</li>
                    </ul>
                </section>
                <section>
                    <h3>ComfyUI-Marigold</h3>
                    <ul>
                        <li><strong>Model:</strong> Marigold</li>
                        <li><strong>Features:</strong> Detailed, flexible, eliminates biases</li>
                        <li><strong>Options:</strong> n_repeat, n_repeat_batch_size</li>
                        <li><strong>Use Case:</strong> High-quality depth for VFX/3D modeling</li>
                    </ul>
                </section>
                <section>
                    <h3>Other Models</h3>
                    <ul>
                        <li>ComfyUI-Depth-Pro (ML-Depth-Pro)</li>
                        <li>ComfyUIDepthEstimation (Transformer models)</li>
                    </ul>
                </section>
            </section>

            <!-- Applications -->
            <section>
                <section>
                    <h2>Practical Applications</h2>
                    <p>Real-world uses of depth estimation</p>
                </section>
                <section>
                    <h3>Image Upscaling</h3>
                    <p>Using depth information to improve detail in upscaled images</p>
                </section>
                <section>
                    <h3>Historical Photo Restoration</h3>
                    <p>Adding depth to bring historical photos to life</p>
                </section>
                <section>
                    <h3>Creative Applications</h3>
                    <p>Artistic and storytelling uses of depth information</p>
                </section>
            </section>

            <!-- Q&A and Resources -->
            <section>
                <h2>Questions?</h2>
                <p>Visit our resources page for more information</p>
                <a href="../hub.html" class="back-button"><i class="fas fa-home"></i> Back to Hub</a>
            </section>
        </div>
    </div>

    <script src="reveal.js/dist/reveal.js"></script>
    <script src="reveal.js/plugin/notes/notes.js"></script>
    <script src="reveal.js/plugin/markdown/markdown.js"></script>
    <script src="reveal.js/plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            plugins: [RevealMarkdown, RevealHighlight, RevealNotes]
        });
    </script>
</body>

</html>